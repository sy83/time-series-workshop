
<!DOCTYPE html>
<html lang="en">

<head>
    <link rel="shortcut icon" href="favicon.ico?">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="NIPS workshop on Learning with Tensors, co-located with NIPS 2016, Barcelona, Spain">


    <title> Tensor-Learn </title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/agency.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

  
</head>

<body id="page-top" class="index">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>

            </div> 

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-middle">
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#introduction">Introduction</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#program">Schedule</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#keynote">Keynote Speakers</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#papers">Accepted Papers</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#call">CFP</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#dates">Important Dates</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#organization">Organizers</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

    <!-- Header -->
    <header>
        <div class="container">
            <div class="intro-text">
            <div class="intro-heading"> Learning with Tensors: <br> Why Now and How ?</br></div>
                            <div class="intro-lead-in"> Tensor-Learn Workshop @ <a href="https://nips.cc/Conferences/2016" target=_blank>NIPS'16</a></br>Dec 10th, 2016 - Barcelona, Spain</div>
                <!--a href="#call" target=_blank class="page-scroll btn btn-xl">Submit a Paper</a-->
            </div>
        </div>
    </header>

    <!-- Introduction Section -->
    <section id="introduction">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Introduction</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>
            <div class="row text-justify">
                <div class="col-md-12">
                    <p class="large text-muted">
                    Real world data in many domains is multimodal and heterogeneous, such as healthcare, social media, and climate science. Tensors, as generalizations of vectors and matrices, provide a natural and scalable framework for handling data with inherent structures and complex dependencies.  Recent renaissance of tensor methods in machine learning ranges from academic research on scalable algorithms for tensor operations, novel models through tensor representations, to industry solutions including Google TensorFlowï¼ŒTorch and Tensor Processing Unit (TPU).  In particular, scalable tensor methods have attracted considerable amount of attention, with successes in a series of learning tasks, such as learning latent variable models, relational learning, spatio-temporal forecasting and training deep neural networks. </br> 
                    </p>
                    <p class ="large text-muted"> 
                    These progresses trigger new directions and problems towards tensor methods in machine learning. The workshop aims to foster discussion, discovery, and dissemination of research activities and outcomes in this area and encourages breakthroughs. We will bring together researchers in theories and applications who are interested in tensors analysis and development of tensor-based algorithms. We will also invite researchers from related areas, such as numerical linear algebra, high-performance computing, deep learning, statistics, data analysis, and many others, to contribute to this workshop. We believe that this workshop can foster new directions, closer collaborations and novel applications. We also expect a deeper conversation regarding why learning with tensors at current stage is important, where it is useful, what tensor computation software and hardware work well in practice and, how we can progress further with interesting research directions and open problems.
                  </p>
                </div>
            </div>
        </div>
    </section>

    <!-- Program Section -->
    <section id="program" class="bg-mid-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Schedule</h2>
                    <h3 class="section-subheading text-muted">
                      Imperial Ballroom A & B <br>
                      8:00 am - 6:30 pm <br>
                      December 10, 2016<br>
                    </h3>
                </div>
            </div>


             
       <div id="schedule">
            <div class = "container w">
                 <h3>Morning Session</h3> <br>
                 <div class = "row centered">
                     <table class = "table table-striped" border="2" cellpadding="2" cellspacing="2">
                         <tbody>
                             <tr>
                                 <td>8:30 - 8:40 </td>
                                 <td>Openning Remarks</td>
                                 <td> </td>
                             </tr>
                             <tr>
                                 <td>8:40 - 9:20 </td>
                                 <td>Invited Talk: Amnon Shashua</td>
                                 <td> </td>
                             </tr>
                             <tr>
                                 <td>9:20 - 10:00 </td>
                                 <td>Contributed Talk</td>
                                 <td> </td>
                             </tr>
                            <tr>
                                 <td>10:00 - 10:30 </td>
                                 <td>Poster Spotlight 1</td>
                                 <td> </td>
                             </tr>
                             <tr>
                                 <td>10:30 - 11:00 </td>
                                 <td>Coffee Break and Poster Session 1 </td>
                                 <td> </td>
                             </tr>
                             <tr>
                                 <td>11:00 - 11:40 </td>
                                 <td>Invited Talk: Lek-Heng Lim</td>
                                 <td> </td>
                             </tr>
                             <tr>
                                 <td>11:40 - 12:20 </td>
                                 <td>Invited Talk: Jimeng Sun</td>
                                 <td> </td>
                             </tr>
                            <tr>
                                 <td>11:20 - 14:00 </td>
                                 <td>Lunch</td>
                                 <td> </td>
                             </tr>
                             
                         </tbody>
                     </table>
                 </div>
                 <h3>Afternoon Session</h3> <br>
                 <div class = "row centered">
                     <table class = "table table-striped" border="2" cellpadding="2" cellspacing="2">
                         <tbody>
                             <tr>
                                 <td>14:00 - 14:40 </td>
                                 <td>Invited Talk: Gregory Valiant</td>
                                 <td> </td>
                             </tr>
                             <tr>
                                 <td>14:40 - 15:00 </td>
                                 <td>Poster Spotlight 2 </td>
                                 <td> </td>
                             </tr>
                             <tr>
                                 <td>15:00 - 15:30</td>
                                 <td>Coffee Break and Poster Session 2 </td>
                                 <td> </td>
                             </tr>
                             <tr>
                                 <td>15:30 - 16:10 </td>
                                 <td>Invited Talk: Vagelis Papalexias</td>
                                 <td> </td>
                             </tr>
                             <tr>
                                 <td>16:10 - 17:00 </td>
                                 <td>PhD Symposium</td>
                                 <td> </td>
                             </tr>
                             <tr>
                                 <td>17:00 - 18:00 </td>
                                 <td>Panel Discussion and Closing Remarks</td>
                                 <td> </td>
                             </tr>
                         </tbody>
                     </table>
                 </div>

              <div class="col-lg-1 text-left">
                &nbsp;
              </div>
          </div>
        </div>
    </section>

    <!-- Keynote Section -->

    <section id="keynote">
             <div class="container">
                 <div class="row">
                     <div class="col-lg-12 text-center">
                         <h2 class="section-heading">Keynote Speakers</h2>
                         <!--h3 class="section-subheading text-muted">More keynote speakers will be announced soon!</h3-->
                     </div>
                 </div>
                 
                 
                 <div class="row">
                     <div class="col-sm-4">
                         <div class="team-member">
                             <img src="img/amnon.jpg" height="150" width="150" class="img-responsive img-circle" alt="Latifur Khan">
                             <a href="http://www.cs.huji.ac.il/~shashua/"><h4>Amnon Shashua</h4></a>
                             <p class="text-muted">Professor<br/>The Hebrew University of Jerusalem</p>
                         </div>
                     </div>
                     <div class="col-sm-8 text-left">
                         
                       <h3  >On Depth Efficiency of Convolutional Networks: the use of Hierarchical Tensor Decomposition for Network Design and Analysis</h3>  
                        <div class="container, col-sm-12 text-left">
                          <a type="button" class="btn btn-info" data-toggle="collapse" data-target="#amnon">Abstract</a>
                          <div id="amnon" class="collapse , text-muted" >
                            Our formal understanding of the inductive bias that drives the success of deep convolutional networks on computer vision tasks is limited. In particular, it is unclear what makes hypotheses spaces born from convolution and pooling operations so suitable for natural images.
I will present recent work that derive an equivalence between convolutional networks and hierarchical tensor decompositions. Under this equivalence, the structure of a network corresponds to the type of decomposition, and the network weights correspond to the decomposition parameters. This allows analyzing hypotheses spaces of networks by studying tensor spaces of corresponding decompositions, facilitating the use of algebraic and measure theoretical tools.  
Specifically, the results I will present  include showing how exponential depth efficiency is achieved in a family of deep networks called Convolutional Arithmetic Circuits, show that CAC is equivalent to SimNets, show that depth efficiency is superior to conventional ConvNets and show how inductive bias is tied to correlations between regions of the input image. In particular, 
correlations are formalized through the notion of separation rank, which for a given input partition, measures how far a function is from being separable.
I will show that a polynomially sized deep network supports exponentially high separation ranks for certain input partitions, while being limited to polynomial separation ranks for others.
The network's pooling geometry effectively determines which input partitions are favored, thus serves as a means for controlling the inductive bias.
Contiguous pooling windows as commonly employed in practice favor interleaved partitions over coarse ones, orienting the inductive bias towards the statistics of natural images.
In addition to analyzing deep networks, I will show that shallow ones support only linear separation ranks, and by this gain insight into the benefit of functions brought forth by depth -- they are able to efficiently model strong correlation under favored partitions of the input.

This work covers material recently presented in COLT, ICML and CVPR including recent Arxiv submissions. The work was jointly done with doctoral students Nadav Cohen and Or Sharir.
<hr>

                          </div>
                        </div>
                         
                      <div class="container, col-sm-12 text-left">
                        <p>Prof. Amnon Shashua holds the Sachs chair in computer science at the Hebrew University of Jerusalem. His field of expertise is computer vision and machine learning. For his academic achievements he received the MARR prize Honorable Mention in 2001, the Kaye innovation award in 2004, and the Landau award in exact sciences in 2005.</p>
        <p>In 1999 Prof. Shashua co-founded Mobileye, an Israeli company developing a system-on-chip and computer vision algorithms for a driving assistance system, providing a full range of active safety features using a single camera. Today, approximately 11 million cars from 25 automobile manufacturers rely on Mobileye technology to make their vehicles safer to drive. In August 2014, Mobileye claimed the title for largest Israeli IPO ever, by raising $1B at a market cap of $5.3B. In addition, Mobileye is developing autonomous driving technology with more than a dozen car manufacturers. An early version of Mobileyeâ€™s autonomous driving technology was deployed in series as an "autopilot" feature in October, 2015, and will evolve to support more autonomous features in 2016 and beyond. The introduction of autonomous driving capabilities is of a transformative nature and has the potential of changing the way cars are built, driven and own in the future.</p>
        <p>In 2010 Prof. Shashua co-founded OrCam which harnesses computer vision artificial intelligence to assist people who are visually impaired or blind. The OrCam MyEye device is unique in its ability to provide visual aid to hundreds of millions of people, through a discreet wearable platform. Within its wide-ranging scope of capabilities, OrCamâ€™s device can read most texts (both indoors and outdoors) and learn to recognize thousands of new items and faces.</p><br>
                         </div>
                     </div>
                 </div>  
                
                <div class="row">
                     <div class="col-sm-4">
                         <div class="team-member">
                             <img src="img/jimeng.jpg" height="150" width="150" class="img-responsive img-circle" alt="Latifur Khan">
                             <a href="http://www.sunlab.org/"><h4>Jimeng Sun</h4></a>
                             <p class="text-muted">Associate Professorr<br/>Georgia Institute of Technology</p>
                         </div>
                     </div>
                     <div class="col-sm-8 text-left">
                       <h3>Computational Phenotyping using Tensor Factorization</h3>
                       <p>Jimeng Sun is an Associate Professor of School of Computational Science and Engineering at College of Computing at Georgia Institute of Technology. His research focuses on medical informatics, especially in applying large-scale predictive modeling and similarity analytics on biomedical applications. </p>
<p>Dr. Sun has extensive research records on data mining: big data analytics, similarity metric learning, social network analysis, predictive modeling, tensor analysis, and visual analytics. He also applies data mining to healthcare applications such as heart failure onset prediction and hypertension control management.</p>
<p>He has published over 70 papers, filed over 20 patents (5 granted). He has received ICDM best research paper in 2008, SDM best research paper in 2007, and KDD Dissertation runner-up award in 2008. Dr. Sun received his B.S. and M.Phil. in Computer Science from Hong Kong University of Science and Technology in 2002 and 2003, and PhD in Computer Science in Carnegie Mellon University in 2007. Prior to joining Georgia Tech, He was a research staff member at IBM TJ Watson Research Center.</p><br>
                        </p>
                     </div>
                 </div>       
 
        
        
        
        
                  <div class="row">
                     <div class="col-sm-4">
                         <div class="team-member">
                             <img src="img/lek-heng.jpg" height="150" width="150" class="img-responsive img-circle" alt="Latifur Khan">
                             <a href="https://www.stat.uchicago.edu/~lekheng/"><h4>Lek-Heng Lim</h4></a>
                             <p class="text-muted">Assistant Professor<br/>University of Chicago</p>
                         </div>
                     </div>
                     <div class="col-sm-8 text-left">
                       <h3>Tensor network ranks</h3>
                       <p>Lek-Heng Lim is an Assistant Professor in the Computational and Applied Mathematics Initiative, the Department of Statistics, and the College of University of Chicago. His research focuses on tensors and their coordinate representations, hypermatrices. He is interested in the hypermatrix equivalents of various matrix notions, their mathematical and computational properties, and their applications to science and engineering. Another area of Lim's interests is applied/computational algebraic and differential geometry, particularly Hodge Laplacians and geometry of subspaces. Lim is also generally interested in numerical linear algebra, optimization and machine learning. </p>
                         <p>Lim was educated at Stanford University (PhD), Cambridge University, Cornell University (MS), and the National University of Singapore (BS). Prior to joining the University of Chicago as an Assistant Professor, he was the Charles Morrey Assistant Professor at UC Berkeley. Lim serves on the editorial boards of Linear Algebra and its Applications and Linear and Multilinear Algebra. His work is supported by an AFOSR Young Investigator Award, an NSF Early Career Award, and a DARPA Young Faculty Award. </p><br>
                        </p>
                     </div>
                 </div>  
                 
               <div class="row">
                     <div class="col-sm-4">
                         <div class="team-member">
                             <img src="img/gregory.jpg" height="150" width="150" class="img-responsive img-circle" alt="Latifur Khan">
                             <a href="http://theory.stanford.edu/~valiant/"><h4>Gregory Valiant</h4></a>
                             <p class="text-muted">Assistant Professor<br/>Stanford</p>
                         </div>
                     </div>
                     <div class="col-sm-8 text-left">
                       <h3>Orthogonalized Alternating Least Squares: A theoretically principled tensor factorization algorithm for practical use</h3>
                         
                        <div class="container, col-sm-12 text-left">
                          <a type="button" class="btn btn-info" data-toggle="collapse" data-target="#greg">Abstract</a>
                          <div id="greg" class="collapse, text-muted">
         From a theoretical perspective, low-rank tensor factorization is an
algorithmic miracle, allowing for (provably correct) reconstruction
and learning in a number of settings.  From a practical standpoint, we
still lack sufficiently robust, versatile, and efficient tensor
factorization algorithms, particularly for large-scale problems.  Many
of the algorithms with provable guarantees either suffer from an
expensive initialization step, and require the iterative removal of
rank-1 factors, destroying any sparsity that might be present in the
original tensor.  On the other hand, the most commonly used algorithm
in practice is "alternating least squares" [ALS], which iteratively
fixes all but one mode, and optimizes the remaining mode.  This
algorithm is extremely efficient, but often converges to bad local
optima, particularly when the weights of the factors are non-uniform.
In this work, we propose a modification of the ALS approach that
enjoys practically viable efficiency, as well as provable recovery
(assuming the factors are random or have small pairwise inner
products) even for highly non-uniform weights.  We demonstrate the
significant superiority of our recovery algorithm over the traditional
ALS on both random synthetic data, and on computing word embeddings
from a third-order word tri-occurrence tensor.

This is based on joint work with Vatsal Sharan.
                              <hr>
                          </div>
                        </div>
                         
                       <p>Greg Valiant is an Assistant Professor in the Computer Science Department at Stanford, after completing a postdoc at Microsoft Research, New England. His main research interests are in algorithms, learning, applied probability and statistics; he is also interested in game theory, and has enjoyed working on problems in database theory.</p>
                        <p> Valiant graduated from Harvard with a BA in Math and an MS in Computer Science, and obtained his PhD in Computer Science from UC Berkeley in 2012. </p><br>
                        </p>
                     </div>
                 </div>  
        
        
                <div class="row">
                     <div class="col-sm-4">
                         <div class="team-member">
                             <img src="img/vagelis.jpg" height="150" width="150" class="img-responsive img-circle" alt="Latifur Khan">
                             <a href="https://www.cs.cmu.edu/~epapalex/"><h4>Vagelis Papalexakis</h4></a>
                             <p class="text-muted">Assistant Professorr<br/>UC Riverside </p>
                         </div>
                     </div>
                     <div class="col-sm-8 text-left">
                       <h3>Tensor decompositions for big multi-aspect data analytics</h3>
                         <div class="container, col-sm-12 text-left">
                          <a type="button" class="btn btn-info" data-toggle="collapse" data-target="#vagelis">Abstract</a>
                          <div id="vagelis" class="collapse, text-muted" >Tensors and tensor decompositions have been very popular and effective tools for analyzing
multi-aspect data in a wide variety of fields, ranging from Psychology to Chemometrics, 
and from Signal Processing to Data Mining and Machine Learning. 

Using tensors in the era of big data poses the challenge of scalability and efficiency. 
In this talk, I will discuss recent techniques on tackling this challenge
by parallelizing and speeding up tensor decompositions, especially
for very sparse datasets (such as the ones encountered for example 
in online social network analysis). 

In addition to scalability, I will also touch upon the challenge of 
unsupervised quality assessment, where in absence of ground truth, we seek
to automatically select the decomposition model that captures best the structure
in our data. 

The talk will conclude with a discussion on future research directions
and open problems in tensors for big data analytics.  
                     <hr>

                          </div>
                        </div>
                         
                        
                         
                         
                         <p>Evangelos (Vagelis) Papalexakis is an Assistant Professor of the CSE Dept. at University of California Riverside. He obtained his PhD degree at the School of Computer Science at Carnegie Mellon University (CMU), under the supervision of Prof. Christos Faloutsos since August 2011. Prior to joining CMU, he obtained his Diploma and MSc in Electronic & Computer Engineering at the Technical University of Crete, in Greece. </p>
                         <p> Broadly, his research interests span the fields of Data Mining, Machine Learning, and Signal Processing. His research involves designing scalable algorithms for mining large multi-aspect datasets, with specific emphasis on tensor factorization models, and applying those algorithms to a variety of real world multi-aspect data problems. His work has appeared in KDD, ICDM, SDM, ECML-PKDD, WWW, PAKDD, ICDE, ICASSP, IEEE Transactions of Signal Processing, and ACM TKDD.  He has a best student paper award at PAKDD'14, finalist best papers for SDM'14 and ASONAM'13 and he was a finalist for the Microsoft PhD Fellowship and the Facebook PhD Fellowship. Besides his academic experience at CMU, he has industrial research experience working at Microsoft Research Silicon Valley during the summers of 2013 and 2014 and Google Research during the summer of 2015.</p><br>
                        </p>
                     </div>
                </div>     
             </div>
            

    </section>


    <!-- Contributed Talks Section -->
    <section id="papers"  class="bg-mid-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Accepted Papers</h2>
                </div>
            </div>
            <div class="row">
              <div class="col-lg-1 text-justify">
                &nbsp;
              </div>
            <div class="col-lg-10 text-justify">



            <p class="row  large " >
                 <h3 class="section-heading text-center">Morning Session</h3>

                <ul>
                    <li>
                        <h4>Structurally regularised Non-negative Tensor Completion for latent spatio-temporal change detection</h4>
                     	Koh Takeuchi, Yoshinobu Kawahara and Tomoharu Iwata
                    </li>
                    <li>
                        <h4>Searching for optimal patterns in Boolean tensors</h4>
                         Dmitry Ignatov, Sergei O. Kuznetsov, Dmitry Gnatyshak and Jaume Baixeries
                    </li>
                    <li>
                        <h4> Using Tensor Theory to Embed Invariances: A Case Study from Turbulence Modeling</h4>
                        Julia Ling
                    </li>
                    <li>
                        <h4>Multi-Label Learning with Provable Guarantee</h4>
                        Sayantan Dasgupta                    </li>      
                    <li>
                        <h4>Bayesian multi-tensor factorization	</h4>
                        Suleiman Ali Khan, Eemeli LeppÃ¤aho and Samuel Kaski
                    </li>
                    <li>
                        <h4>Tensor Decomposition with Smoothness</h4>
                     	Masaaki Imaizumi and Kohei Hayashi
                    </li>
                    <li>
                        <h4>Non-negative Factorization of the Occurrence Tensor from Financial Contracts</h4>
                         Zheng Xu, Furong Huang, Louiqa Raschid and Tom Goldstein
                    </li>
                    <li>
                        <h4>TensorLy: Tensor Learning in Python	</h4>
                        Jean Kossaifi, Yannis Panagakis and Maja Pantic	            
                    </li>
                    
                    <li>
                        <h4>CoFactor: Concise Factorization of Sparse and High-Order Tensors</h4>
                          Ioakeim Perros, Richard Peng, Richard Vuduc and Jimeng Sun
                    </li>
                    <li>
                        <h4>ParTI: A Parallel Tensor Infrastructure for Data Analysis</h4>
                        Jiajia Li, Yuchen Ma, Chenggang Yan, Jimeng Sun and Richard Vuduc	
                    </li>
                     <li>
                        <h4>SenTenCE: A multi-sensor data compression framework using tensor decompositions for human activity classification</h4>
                        Vinay Uday Prabhu and John Whaley	
                    </li>   
                </ul>
                <ul>
                        <h3 class="section-heading text-center">Afternoon Session</h3>

                    <li>
                        <h4> Ultimate tensorization: convolutions and FC alike</h4>
                        Timur Garipov, Dmitry Podoprikhin, Alexander Novikov and Dmitry Vetrov
                    </li>
                    <li>
                        <h4>BaTFLED: Bayesian Tensor Factorization Linked to External Data</h4>
                        Nathan Lazar, Mehmet Gonen and Kemal Sonmez	                    
                    </li>               
     
                    <li>
                        <h4>Approximate Inference in Graphical Models via Low-Rank Tensor Propagation</h4>
                        Andrew Wrigley, Wee Sun Lee and Nan Ye	
                    </li>
    
                    <li>
                        <h4>Factorizing Sparse Tensors for Supervised Machine Learning</h4>
Stephan Baier and Volker Tresp	
                    </li>      
                    <li>
                        <h4>Graph Learning as a Tensor Factorization Problem</h4>
                        Raphael Bailly and Guillaume Rabusseau	                   
                    </li>
                    <li>
                        <h4>Learning Maliciousness in Cybersecurity Graphs</h4>
                        Akshay Rangamani, Connor Walsh, Sam Gottlieb and Elisabeth Maida	                    
                    </li>
           

               
                 </ul>
            </p>

            </div>
     

           </div>

        </div>
        </section>




    <!-- call for papers -->
    <section id="call">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Call for Papers</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>
            <div class="row text-justify">
                <div class="col-md-12">
                    <p class="large text-muted">
                    Papers submitted to the workshop should be up to four pages long excluding references and in NIPS 2016 format.  As the review process is not blind, authors can reveal their identity in their submissions. All inquiries could be sent to tensorlearn@gmail.com.              
                    <p class="large text-muted">
                    <!--   Submissions website: TBA. -->
                      Submissions page: <a href="https://easychair.org/conferences/?conf=tensorlearn2016">Tensor-Learn 2016</a>.
                  </p> 
                   
                    </p>
                  <p class="large text-muted">
                      <b>Note on <i>open problem</i> submissions:</b> In order to promote new and innovative research on tensors, we plan to accept a small number of high quality manuscripts describing <i>open problems</i> in tensor learning. Such papers should provide a clear, detailed description and analysis of a new or open problem that poses a significant challenge to existing techniques, as well as a thorough empirical investigation demonstrating that current methods are insufficient. Accepted submissions will be presented as posters. there is no published proceedings and the authors are free to send it elsewhere.
                  </p>
  
               </div>
            </div>
        </div>
    </section>

    <!-- Dates Section -->
    <section id="dates" class="bg-mid-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Key Dates</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>
            <div class="row">
              <div class="col-lg-4 text-left">
                &nbsp;
              </div>
            <div class="col-lg-6 text-left">
                <div class="col-md-12">
                      <p class="large text-muted">
                          <b>Paper Submission Deadline:</b> <strike>Oct 28, 2016, 11:59 PM PST</strike>
                    </p><p class="large text-muted">
                      <b>Author Notification:</b> <strike>Nov 7, 2016, 11:59 PM PST</strike>
                    </p><p class="large text-muted">
                      <b>Final Version:</b> <strike>Nov 25, 2016, 11:59 PM PST</strike>
                    </p><p class="large text-muted">
                      <b>Workshop:</b> December 10, 2016
                      </p>
                </div>
            </div>
          </div>
        </div>
    </section>

  <!-- Organization Section -->
    <section id="organization">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Workshop Organizers</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>

                <div class="col-sm-1">
                   &nbsp;
                </div>


                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="img/anima.png" class="img-responsive img-circle" alt="">
                        <a href="http://newport.eecs.uci.edu/anandkumar/"> <h4>Anima Anandkumar</h4> </a>
                        <p class="text-muted">University of California Irvine</p>
                    </div>
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="img/rongge.jpg" class="img-responsive img-circle" alt="">
                        <a href="https://users.cs.duke.edu/~rongge/"> <h4>Rong Ge</h4> </a>
                        <p class="text-muted">Duke University</p>
                    </div>
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="img/yanliu.jpg" class="img-responsive img-circle" alt="">
                        <a href="http://www-bcf.usc.edu/~liu32/"><h4>Yan Liu</h4>  </a>
                        <p class="text-muted">University of Southern California</p>
                    </div>
                </div>
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="img/max.jpg" class="img-responsive img-circle" alt="">
                        <a href="http://www.mit.edu/~mnick/"><h4>Maximilian Nickel</h4></a>
                        <p class="text-muted">Facebook AI Research (FAIR)</p>
                    </div>
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="img/rose.jpg" class="img-responsive img-circle" alt="">
                        <a href="http://roseyu.com"><h4>Rose Yu</h4></a>
                        <p class="text-muted">University of Southern California</p>
                    </div>
                </div>

                <div class="col-sm-1">
                   &nbsp;
                </div>
            </div>
            
        </div>
    </section>




    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="http://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
    <script src="js/classie.js"></script>
    <script src="js/cbpAnimatedHeader.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="js/contact_me.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/agency.js"></script>

</body>

</html>
